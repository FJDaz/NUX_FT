# ğŸ“ Fine-Tuning Mistral 7B - Philosophes (SchÃ¨mes Logiques)

**Objectif :** Fine-tuner Mistral 7B sur 1200 exemples de schÃ¨mes logiques philosophiques pour crÃ©er une version CPU-compatible du systÃ¨me Bergson and Friends.

## ğŸ“Š Datasets

- **Base :** 300 exemples (`data/FT/processed/schemes_levelA_base.jsonl`)
- **AugmentÃ©s :** 900 exemples (`data/FT/processed/schemes_levelA_augmented.jsonl`)
- **Total :** 1200 exemples de schÃ¨mes logiques (Modus Ponens, identitÃ©s spinozistes, causalitÃ©)
- **Format :** ChatML (system/user/assistant)
- **Registre :** LycÃ©en (Terminale)

## ğŸ¯ StratÃ©gie

### ModÃ¨le Base
- **Mistral 7B Instruct v0.3** (`mistralai/Mistral-7B-Instruct-v0.3`)
- Fine-tuning : **QLoRA** (4-bit quantization)
- MÃ©thode : **PEFT/LoRA** (rang 64, alpha 128)

### Use Case
- **Free Tier :** Mistral 7B LoRA sur CPU HF Space (latence 5-15s)
- **Premium Tier :** Qwen 14B LoRA SNB sur GPU Modal (latence <2s)

### QualitÃ© Attendue
- Application rigoureuse des schÃ¨mes logiques
- RÃ©ponses structurÃ©es (prÃ©misses â†’ conclusion)
- Vocabulaire lycÃ©en accessible

## ğŸš€ Training

### Colab Pro (RecommandÃ©)
```bash
# Ouvrir le notebook Colab
notebooks/train_mistral_7b_lora.ipynb

# GPU optimal : A100 40GB
# Temps : 30-45 minutes
# Config : r=64, batch_size=8, epochs=3
```

### Local (Backup)
```bash
# Avec MPS (Apple Silicon) ou CPU
python scripts/train_local.py --config configs/mistral_7b_lora.yaml
```

## ğŸ“ Structure

```
/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ FT/
â”‚       â”œâ”€â”€ Dataset Niveau A Schemes.txt
â”‚       â””â”€â”€ processed/
â”‚           â”œâ”€â”€ schemes_levelA_base.jsonl (300 ex)
â”‚           â””â”€â”€ schemes_levelA_augmented.jsonl (900 ex)
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ train_mistral_7b_lora.ipynb (Colab Pro)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_local.py (training local)
â”‚   â””â”€â”€ test_model.py (benchmarks)
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ mistral_7b_lora.yaml (hyperparams)
â””â”€â”€ models/
    â””â”€â”€ (LoRA checkpoints aprÃ¨s training)
```

## ğŸ“ Philosophes Cibles

- **Spinoza :** SchÃ¨mes d'identitÃ© (Dieu=Nature), causalitÃ© nÃ©cessaire, affects
- **Bergson :** Opposition durÃ©e/temps spatial, mÃ©taphores temporelles
- **Kant :** Distinctions (phÃ©nomÃ¨ne/noumÃ¨ne), conditions transcendantales

## ğŸ“ˆ Benchmarks

AprÃ¨s training, tester sur 10 questions par philosophe :
- Application correcte des schÃ¨mes logiques
- CohÃ©rence style conversationnel lycÃ©en
- Latence CPU (4-bit quantization)

---

**DerniÃ¨re mise Ã  jour :** 20 novembre 2025
