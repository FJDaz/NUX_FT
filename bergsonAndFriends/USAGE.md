# ğŸ“– Guide d'Utilisation - Fine-Tuning Mistral 7B

## ğŸš€ Quick Start

### 1. Training sur Colab Pro

#### Ã‰tape 1 : Uploader les fichiers
1. Ouvrir [Google Colab](https://colab.research.google.com/)
2. Upload le notebook : `notebooks/train_mistral_7b_lora.ipynb`
3. SÃ©lectionner GPU : **Runtime > Change runtime type > A100 GPU** (ou V100/T4)
4. Upload les datasets dans Colab :
   - `data/FT/processed/schemes_levelA_base.jsonl`
   - `data/FT/processed/schemes_levelA_augmented.jsonl`

#### Ã‰tape 2 : Configuration
1. Dans la cellule "Configuration", remplacer :
   ```python
   HF_TOKEN = "hf_..."  # Votre token HF (avec write access)
   HF_USERNAME = "FJDaz"  # Votre username HF
   ```

#### Ã‰tape 3 : Lancer le training
1. Cliquer sur **Runtime > Run all**
2. Attendre la fin du training :
   - **A100 40GB :** 30-45 minutes âš¡
   - **V100 16GB :** 1h-1h30
   - **T4 15GB :** 2-3h

#### Ã‰tape 4 : RÃ©cupÃ©rer le modÃ¨le
**Option A : Download local**
```python
# DerniÃ¨re cellule du notebook
!zip -r mistral-7b-philosophes-lora-final.zip ./mistral-7b-philosophes-lora-final/
# TÃ©lÃ©charger depuis l'explorateur Colab (gauche)
```

**Option B : Push vers HF Hub** (automatique si configurÃ©)
- Votre LoRA sera disponible sur : `https://huggingface.co/FJDaz/mistral-7b-philosophes-lora`

---

## ğŸ§ª Test Local du ModÃ¨le

### PrÃ©requis
```bash
cd /Users/francois-jeandazin/NUX_FT/bergsonAndFriends

# Installer les dÃ©pendances
pip install torch transformers peft bitsandbytes accelerate
```

### Tester le LoRA

```bash
# Copier le LoRA tÃ©lÃ©chargÃ© depuis Colab
unzip mistral-7b-philosophes-lora-final.zip -d models/

# Lancer les benchmarks
python scripts/test_model.py --lora ./models/mistral-7b-philosophes-lora-final

# RÃ©sultats attendus :
# - 10 questions testÃ©es (Spinoza, Bergson, Kant)
# - Accuracy : >80% application correcte schÃ¨mes
# - Latence CPU 4-bit : 5-15s par rÃ©ponse
```

### Comparer avec modÃ¨le base (sans LoRA)

```bash
# Tester sans LoRA (baseline)
python scripts/test_model.py --lora None

# Comparer les rÃ©sultats :
# - LoRA devrait avoir meilleure accuracy sur schÃ¨mes
# - Latence similaire (mÃªme quantization 4-bit)
```

---

## ğŸ“Š RÃ©sultats Attendus

### MÃ©triques Training

| MÃ©trique | Valeur Cible |
|----------|--------------|
| **Training loss (final)** | < 0.5 |
| **Eval loss (final)** | < 0.6 |
| **Temps (A100 40GB)** | 30-45 min |
| **Taille LoRA** | 250-350 MB |
| **ParamÃ¨tres entraÃ®nables** | ~1-2% du total |

### MÃ©triques InfÃ©rence (CPU 4-bit)

| MÃ©trique | Valeur Cible |
|----------|--------------|
| **Accuracy schÃ¨mes** | > 80% |
| **Latence par rÃ©ponse** | 5-15s (CPU) |
| **Latence par rÃ©ponse** | 1-3s (GPU) |
| **RAM requise** | ~4 GB (4-bit) |

---

## ğŸ”§ Troubleshooting

### ProblÃ¨me : "CUDA out of memory" sur Colab

**Solution :**
1. RÃ©duire le batch size dans le notebook :
   ```python
   BATCH_SIZE = 2  # Au lieu de 4 ou 8
   GRADIENT_ACCUM = 16  # Compenser
   ```

2. Ou redÃ©marrer le runtime et choisir un GPU plus puissant

### ProblÃ¨me : Training trop lent (>3h sur T4)

**Solution :**
1. RÃ©duire `num_train_epochs` Ã  2 (au lieu de 3)
2. Ou utiliser seulement `schemes_levelA_base.jsonl` (300 exemples au lieu de 1200)

### ProblÃ¨me : ModÃ¨le ne gÃ©nÃ¨re que du texte gÃ©nÃ©rique

**Solution :**
1. VÃ©rifier que le LoRA a bien Ã©tÃ© chargÃ© :
   ```python
   model = PeftModel.from_pretrained(model, lora_path)
   ```

2. VÃ©rifier le format du prompt (doit inclure schÃ¨me + contexte)

3. VÃ©rifier eval_loss : si > 1.0, le training n'a pas convergÃ© (relancer)

---

## ğŸ“ Structure des Fichiers

### AprÃ¨s Training Colab

```
mistral-7b-philosophes-lora-final/
â”œâ”€â”€ adapter_config.json          # Config LoRA
â”œâ”€â”€ adapter_model.safetensors    # Poids LoRA (~250-350 MB)
â””â”€â”€ tokenizer*                   # Tokenizer (copie)
```

### AprÃ¨s Download Local

```
/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ mistral-7b-philosophes-lora-final/  # LoRA tÃ©lÃ©chargÃ©
â”œâ”€â”€ benchmark_results.json                   # RÃ©sultats tests
â””â”€â”€ ...
```

---

## ğŸ¯ Prochaines Ã‰tapes

### 1. Benchmarks Complets
```bash
# CrÃ©er 30 questions test (10 par philosophe)
# Tester accuracy, latence, qualitÃ© rÃ©ponses
python scripts/test_model.py --lora ./models/mistral-7b-philosophes-lora-final
```

### 2. DÃ©ploiement HF Space CPU

**CrÃ©er un Space HF gratuit :**
1. CrÃ©er repo : `FJDaz/mistral-7b-philosophes-demo`
2. Uploader :
   - `app.py` (Gradio interface)
   - `requirements.txt`
   - Charger LoRA depuis HF Hub
3. Hardware : **CPU basic** (gratuit)
4. Tester latence CPU sur Space

### 3. Comparaison avec Qwen 14B SNB

| CritÃ¨re | Mistral 7B LoRA (CPU) | Qwen 14B LoRA SNB (GPU) |
|---------|----------------------|-------------------------|
| QualitÃ© schÃ¨mes | ğŸŸ¡ Bonne (80-90%) | âœ… Excellente (95%+) |
| Latence | âš ï¸ 5-15s (CPU) | âœ… 1-3s (GPU) |
| CoÃ»t | âœ… Gratuit (HF CPU) | ğŸ’° Pay-per-use (Modal) |
| Use case | Free tier | Premium tier |

---

## ğŸ“ Support

**ProblÃ¨mes ou questions ?**
- Consulter `README.md` pour vue d'ensemble
- Consulter `configs/mistral_7b_lora.yaml` pour hyperparams dÃ©taillÃ©s
- VÃ©rifier `benchmark_results.json` pour mÃ©triques attendues

---

**DerniÃ¨re mise Ã  jour :** 20 novembre 2025
**Auteur :** Claude Code
**Config optimale :** Colab Pro A100 40GB, r=64, batch_size=8, epochs=3
