{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning Mistral 7B - Philosophes (LoRA)\n",
    "\n",
    "**Objectif :** Fine-tuner Mistral 7B sur sch√®mes logiques + dialogues incarn√©s\n",
    "\n",
    "**GPU Optimal :** A100 40GB (30-45 min) | V100 16GB (1h-1h30) | **L4 15GB (2-3h)**\n",
    "\n",
    "**Config :**\n",
    "- Mod√®le : `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "- M√©thode : QLoRA (4-bit) + LoRA (r=64, alpha=128)\n",
    "- Dataset initial : 1200 exemples sch√®mes\n",
    "- Re-fine-tuning : 933 exemples (80% sch√®mes + 20% incarnation)\n",
    "- Epochs : 3 (initial) + 3 (re-fine-tuning)\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è FICHIERS √Ä UPLOADER DANS COLAB :**\n",
    "1. `schemes_levelA_base.jsonl`\n",
    "2. `schemes_levelA_augmented.jsonl`\n",
    "3. `enriched_correction_dataset.jsonl`\n",
    "\n",
    "Upload dans `/content/` (panneau Files √† gauche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup - Installation & V√©rifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances optimis√©es\n",
    "print(\"üì¶ Installation des packages (2-3 minutes)...\\n\")\n",
    "\n",
    "!pip install -U \\\n",
    "    torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 \\\n",
    "    transformers>=4.40.0 \\\n",
    "    peft>=0.10.0 \\\n",
    "    bitsandbytes>=0.43.0 \\\n",
    "    accelerate>=0.28.0 \\\n",
    "    trl>=0.8.0 \\\n",
    "    datasets>=2.18.0 \\\n",
    "    huggingface_hub>=0.22.0\n",
    "\n",
    "print(\"\\n‚úÖ Installation termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # D√©terminer config optimale selon GPU\n",
    "    if \"A100\" in gpu_name and gpu_memory > 35:\n",
    "        print(\"\\nüöÄ GPU OPTIMAL: A100 40GB\")\n",
    "        BATCH_SIZE = 8\n",
    "        GRADIENT_ACCUM = 4\n",
    "    elif \"V100\" in gpu_name or (\"A100\" in gpu_name and gpu_memory < 20):\n",
    "        print(\"\\n‚úÖ GPU EXCELLENT: V100/A100-16GB\")\n",
    "        BATCH_SIZE = 4\n",
    "        GRADIENT_ACCUM = 8\n",
    "    else:\n",
    "        print(f\"\\nüü° GPU STANDARD: {gpu_name}\")\n",
    "        BATCH_SIZE = 2\n",
    "        GRADIENT_ACCUM = 16\n",
    "    \n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Gradient accumulation: {GRADIENT_ACCUM}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA non disponible - Training impossible\")\n",
    "    BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUM = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration - Authentification HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentification Hugging Face\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# Option 1: Secrets Colab (RECOMMAND√â)\n",
    "try:\n",
    "    HF_TOKEN = userdata.get('HF_TOKEN')\n",
    "    print(\"‚úÖ Token HF r√©cup√©r√© depuis Colab Secrets\")\n",
    "except:\n",
    "    # Option 2: Saisie manuelle\n",
    "    print(\"‚ö†Ô∏è Token HF_TOKEN non trouv√© dans Colab Secrets\")\n",
    "    print(\"üìù Configuration Colab Secrets:\")\n",
    "    print(\"   1. Ic√¥ne üîë (barre gauche)\")\n",
    "    print(\"   2. Ajouter: Nom=HF_TOKEN, Valeur=votre_token\")\n",
    "    print(\"   3. Activer l'acc√®s + red√©marrer\\n\")\n",
    "    \n",
    "    from getpass import getpass\n",
    "    HF_TOKEN = getpass(\"Token HF (https://huggingface.co/settings/tokens): \")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "print(\"‚úÖ Authentification HF r√©ussie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Dataset Initial - Sch√®mes Logiques (1200 exemples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger datasets sch√®mes logiques\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "print(\"üì• Chargement datasets sch√®mes logiques...\")\n",
    "\n",
    "dataset_base = load_dataset('json', data_files='schemes_levelA_base.jsonl', split='train')\n",
    "dataset_augmented = load_dataset('json', data_files='schemes_levelA_augmented.jsonl', split='train')\n",
    "\n",
    "dataset_full = concatenate_datasets([dataset_base, dataset_augmented])\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√©: {len(dataset_full)} exemples\")\n",
    "print(f\"   - Base: {len(dataset_base)}\")\n",
    "print(f\"   - Augment√©s: {len(dataset_augmented)}\")\n",
    "\n",
    "print(\"\\nüìù Exemple:\")\n",
    "print(dataset_full[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation (95/5)\n",
    "dataset_split = dataset_full.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(f\"‚úÖ Split:\")\n",
    "print(f\"   - Train: {len(train_dataset)}\")\n",
    "print(f\"   - Validation: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Mod√®le - Configuration QLoRA + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Configuration quantization 4-bit\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(\"üì• Chargement Mistral 7B (4-bit)...\")\n",
    "print(\"‚è≥ Ceci prend 5-10 min (t√©l√©chargement + quantization)\\n\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© en 4-bit\")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "print(\"‚úÖ Pr√©par√© pour k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration LoRA (r=64 pour haute qualit√©)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA appliqu√© (r={lora_config.r}, alpha={lora_config.lora_alpha})\")\n",
    "print(f\"   Param√®tres entra√Ænables: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "print(f\"   Param√®tres totaux: {total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training Initial - Sch√®mes Logiques (3 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formater datasets avec champ \"text\"\n",
    "def format_chat_template(example):\n",
    "    messages = example['messages']\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_dataset_formatted = train_dataset.map(format_chat_template, remove_columns=train_dataset.column_names)\n",
    "eval_dataset_formatted = eval_dataset.map(format_chat_template, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "print(f\"‚úÖ Datasets format√©s\")\n",
    "print(f\"   Train: {len(train_dataset_formatted)}\")\n",
    "print(f\"   Eval: {len(eval_dataset_formatted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-7b-philosophes-lora\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=3,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=eval_dataset_formatted,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√©\")\n",
    "print(f\"   Effective batch: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "print(f\"   Steps estim√©s: ~{len(train_dataset_formatted) * 3 // (BATCH_SIZE * GRADIENT_ACCUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ LANCER LE TRAINING INITIAL\n",
    "print(\"üöÄ Training initial (sch√®mes logiques)\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if \"A100\" in torch.cuda.get_device_name(0) and torch.cuda.get_device_properties(0).total_memory > 35e9:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 30-45 min (A100 40GB)\")\n",
    "elif \"V100\" in torch.cuda.get_device_name(0):\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 1h-1h30 (V100)\")\n",
    "else:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 2-3h (L4/T4)\")\n",
    "\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training initial termin√© !\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Sauvegarde Checkpoint Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder localement\n",
    "output_dir = \"./mistral-7b-philosophes-lora-final\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√©: {output_dir}\")\n",
    "\n",
    "import os\n",
    "lora_size = sum(os.path.getsize(os.path.join(output_dir, f)) for f in os.listdir(output_dir)) / 1024**2\n",
    "print(f\"   Taille LoRA: {lora_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Test Initial - V√©rification Sch√®mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide application sch√®mes\n",
    "import torch\n",
    "\n",
    "print(\"üìù Test: Modus Ponens spinoziste\\n\")\n",
    "\n",
    "test_prompt = \"\"\"Sch√®me : Modus Ponens\n",
    "Contexte : Si l'homme ignore les causes de ses passions, il est en servitude. Or l'√©l√®ve ignore les causes de ses passions.\n",
    "Applique le sch√®me :\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un tuteur philosophique ma√Ætrisant les sch√®mes logiques.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=128, do_sample=True, temperature=0.7, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "response = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\n",
    "print(f\"R√©ponse: {response}\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ RE-FINE-TUNING - Dataset Combin√© (80% sch√®mes + 20% incarnation)\n",
    "\n",
    "**Objectif :** Ajouter l'incarnation en 1√®re personne SANS oublier les sch√®mes logiques\n",
    "\n",
    "**Strat√©gie :** Combiner 80% sch√®mes + 100% incarnation ‚Üí ~933 exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger dataset combin√©\n",
    "print(\"üîÑ Pr√©paration dataset combin√© (80/20)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Charger datasets\n",
    "print(\"\\nüì• Chargement...\")\n",
    "dataset_schemes = load_dataset('json', data_files='schemes_levelA_augmented.jsonl', split='train')\n",
    "dataset_incarnation = load_dataset('json', data_files='enriched_correction_dataset.jsonl', split='train')\n",
    "\n",
    "print(f\"‚úÖ Charg√©s:\")\n",
    "print(f\"   - Sch√®mes: {len(dataset_schemes)}\")\n",
    "print(f\"   - Incarnation: {len(dataset_incarnation)}\")\n",
    "\n",
    "# Ratio 80/20\n",
    "num_schemes = int(len(dataset_schemes) * 0.8)\n",
    "dataset_schemes_sample = dataset_schemes.shuffle(seed=42).select(range(num_schemes))\n",
    "\n",
    "# Combiner\n",
    "dataset_combined = concatenate_datasets([dataset_schemes_sample, dataset_incarnation])\n",
    "dataset_combined = dataset_combined.shuffle(seed=42)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset combin√©: {len(dataset_combined)} exemples\")\n",
    "print(f\"   Ratio: {100*num_schemes/len(dataset_combined):.1f}% sch√®mes / {100*len(dataset_incarnation)/len(dataset_combined):.1f}% incarnation\")\n",
    "\n",
    "# Split\n",
    "split = dataset_combined.train_test_split(test_size=0.05, seed=42)\n",
    "train_combined = split['train']\n",
    "eval_combined = split['test']\n",
    "\n",
    "print(f\"   Train: {len(train_combined)}\")\n",
    "print(f\"   Eval: {len(eval_combined)}\")\n",
    "\n",
    "# Formater\n",
    "train_combined = train_combined.map(format_chat_template, remove_columns=train_combined.column_names)\n",
    "eval_combined = eval_combined.map(format_chat_template, remove_columns=eval_combined.column_names)\n",
    "\n",
    "print(\"\\n‚úÖ Datasets format√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config training combin√©\n",
    "training_args_combined = TrainingArguments(\n",
    "    output_dir=\"./mistral-combined\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
    "    learning_rate=2e-4,  # LR normal\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    num_train_epochs=3,  # 3 epochs\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    logging_steps=5,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_steps=50,\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer_combined = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args_combined,\n",
    "    train_dataset=train_combined,\n",
    "    eval_dataset=eval_combined,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer combin√© cr√©√©\")\n",
    "print(f\"   LR: 2e-4 (normal)\")\n",
    "print(f\"   Epochs: 3\")\n",
    "print(f\"   Monitoring: eval_loss/20 steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ LANCER RE-FINE-TUNING\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ RE-FINE-TUNING (dataset combin√©)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚è±Ô∏è Temps estim√©: ~1-1.5h (L4)\")\n",
    "print(\"\\nüí° MONITORING:\")\n",
    "print(\"   - eval_loss doit diminuer\")\n",
    "print(\"   - Si remonte ‚Üí overfitting\")\n",
    "print(\"   - Objectif: eval_loss < 0.6\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "trainer_combined.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ RE-FINE-TUNING termin√© !\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Metrics finales\n",
    "final = trainer_combined.state.log_history[-1]\n",
    "print(f\"\\nüìä METRICS:\")\n",
    "if 'loss' in final:\n",
    "    print(f\"   Train loss: {final['loss']:.4f}\")\n",
    "if 'eval_loss' in final:\n",
    "    print(f\"   Eval loss: {final['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde finale\n",
    "output_final = \"./mistral-combined-final\"\n",
    "trainer_combined.model.save_pretrained(output_final)\n",
    "tokenizer.save_pretrained(output_final)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le final sauvegard√©: {output_final}\")\n",
    "\n",
    "# Push HF\n",
    "SPACE_REPO = \"FJDaz/3_PHI\"\n",
    "\n",
    "print(f\"\\nüì§ Push vers {SPACE_REPO}/Spinoza_Secours...\")\n",
    "\n",
    "trainer_combined.model.push_to_hub(\n",
    "    SPACE_REPO,\n",
    "    subfolder=\"Spinoza_Secours\",\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    commit_message=\"Mistral 7B LoRA (80% schemas + 20% incarnation)\"\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    SPACE_REPO,\n",
    "    subfolder=\"Spinoza_Secours\",\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Push√©: https://huggingface.co/spaces/{SPACE_REPO}/tree/main/Spinoza_Secours\")\n",
    "print(\"\\nüí° Structure:\")\n",
    "print(\"   FJDaz/3_PHI/\")\n",
    "print(\"   ‚îú‚îÄ‚îÄ qwen-spinoza-niveau-b/  ‚Üê SNB (inchang√©)\")\n",
    "print(\"   ‚îî‚îÄ‚îÄ Spinoza_Secours/        ‚Üê Mistral 7B (nouveau)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Test Final - Dialogue Interactif\n",
    "\n",
    "**V√©rifications :**\n",
    "- ‚úÖ Parle en 1√®re personne (\"Je montre\" pas \"Spinoza montre\")\n",
    "- ‚úÖ Applique sch√®mes logiques\n",
    "- ‚úÖ Pas de r√©p√©tition\n",
    "- ‚úÖ Tags: `[D√âTECTION: ACCORD]` (sans espace)\n",
    "- ‚úÖ R√©pond au contexte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialogue interactif multi-tours\n",
    "import torch\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Tu ES Spinoza incarn√©. Tu dialogues avec un √©l√®ve de Terminale en premi√®re personne.\n",
    "\n",
    "R√àGLES STRICTES:\n",
    "- Tutoie toujours l'√©l√®ve (tu/ton/ta)\n",
    "- Reste concis (2-3 phrases MAX)\n",
    "- Questionne au lieu d'affirmer\n",
    "- Varie tes formulations\n",
    "- Ne parle JAMAIS de toi √† la 3√®me personne. Tu ES Spinoza.\n",
    "- R√©ponds √† la question pos√©e, pas √† une question pr√©c√©dente.\n",
    "- Adapte ta r√©ponse au contexte imm√©diat de la conversation.\"\"\"\n",
    "\n",
    "def generate_response(conversation):\n",
    "    inputs = tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True).strip()\n",
    "\n",
    "# Init conversation\n",
    "conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "\n",
    "print(\"üí¨ DIALOGUE AVEC SPINOZA\")\n",
    "print(\"=\"*60)\n",
    "print(\"Commandes: 'exit' (quitter), 'reset' (recommencer)\\n\")\n",
    "\n",
    "test_questions = [\n",
    "    \"La libert√©, c'est faire ce qu'on veut ?\",\n",
    "    \"Ben, c'est pas toi Spinoza ?\",\n",
    "    \"Tu veux dire que si je comprends pourquoi je suis frustr√©, je me sens plus libre ?\",\n",
    "]\n",
    "\n",
    "print(\"üí° QUESTIONS TEST:\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "turn = 0\n",
    "while True:\n",
    "    turn += 1\n",
    "    user_input = input(f\"[{turn}] üë§ VOUS : \")\n",
    "    \n",
    "    if user_input.lower() == 'exit':\n",
    "        print(\"\\nüëã Au revoir !\")\n",
    "        break\n",
    "    \n",
    "    if user_input.lower() == 'reset':\n",
    "        conversation = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "        turn = 0\n",
    "        print(\"\\nüîÑ Conversation r√©initialis√©e\\n\")\n",
    "        continue\n",
    "    \n",
    "    if not user_input.strip():\n",
    "        continue\n",
    "    \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    \n",
    "    print(\"ü§î Spinoza r√©fl√©chit...\", end=\"\", flush=True)\n",
    "    response = generate_response(conversation)\n",
    "    print(f\"\\rüí¨ SPINOZA : {response}\\n\")\n",
    "    \n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response})\n",
    "    \n",
    "    # V√©rifications\n",
    "    issues = []\n",
    "    if \"Spinoza\" in response and (\"Pour Spinoza\" in response or \"montre que\" in response.split(\"Spinoza\")[1][:50]):\n",
    "        issues.append(\"‚ö†Ô∏è 3√®me personne\")\n",
    "    if \"[D√âTECTION : \" in response:\n",
    "        issues.append(\"‚ö†Ô∏è Tag malform√©\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"   {'  '.join(issues)}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"‚úÖ Dialogue termin√© ({turn} tours)\")\n",
    "print(\"\\nüí° Pour voir l'historique: print(conversation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checklist Finale\n",
    "\n",
    "- [ ] Training initial termin√© (eval_loss < 0.6)\n",
    "- [ ] Re-fine-tuning combin√© termin√© (eval_loss < 0.6)\n",
    "- [ ] Test dialogue: parle en 1√®re personne\n",
    "- [ ] Test dialogue: applique sch√®mes correctement\n",
    "- [ ] Test dialogue: pas de r√©p√©tition\n",
    "- [ ] Mod√®le push√© sur HF: `FJDaz/3_PHI/Spinoza_Secours`\n",
    "\n",
    "---\n",
    "\n",
    "**Cr√©√© le :** 20 novembre 2025  \n",
    "**Version :** CLEAN v2 (dataset combin√© 80/20)  \n",
    "**Mod√®le :** Mistral 7B Instruct v0.3 + LoRA r=64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
