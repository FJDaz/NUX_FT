{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning Mistral 7B - Philosophes (LoRA)\n",
    "\n",
    "**Objectif :** Fine-tuner Mistral 7B sur 1200 exemples de sch√®mes logiques philosophiques\n",
    "\n",
    "**GPU Optimal :** A100 40GB (30-45 min) | V100 16GB (1h-1h30) | T4 15GB (2-3h)\n",
    "\n",
    "**Config :**\n",
    "- Mod√®le : `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "- M√©thode : QLoRA (4-bit) + LoRA (r=64, alpha=128)\n",
    "- Dataset : 1200 exemples (300 base + 900 augment√©s)\n",
    "- Epochs : 3\n",
    "- Batch size : 8 (A100) | 4 (V100/T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup - Installation & V√©rifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation des d√©pendances optimis√©es (versions r√©centes)\n# Note: Versions sp√©cifiques pour √©viter conflits avec torchaudio/torchvision\nprint(\"üì¶ Installation des packages (peut prendre 2-3 minutes)...\\n\")\n\n!pip install -U \\\n    torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 \\\n    transformers>=4.40.0 \\\n    peft>=0.10.0 \\\n    bitsandbytes>=0.43.0 \\\n    accelerate>=0.28.0 \\\n    trl>=0.8.0 \\\n    datasets>=2.18.0 \\\n    huggingface_hub>=0.22.0\n\nprint(\"\\n‚úÖ Installation termin√©e !\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # D√©terminer config optimale selon GPU\n",
    "    if \"A100\" in gpu_name and gpu_memory > 35:\n",
    "        print(\"\\nüöÄ GPU OPTIMAL d√©tect√©: A100 40GB\")\n",
    "        BATCH_SIZE = 8\n",
    "        GRADIENT_ACCUM = 4\n",
    "    elif \"V100\" in gpu_name or (\"A100\" in gpu_name and gpu_memory < 20):\n",
    "        print(\"\\n‚úÖ GPU EXCELLENT d√©tect√©: V100/A100-16GB\")\n",
    "        BATCH_SIZE = 4\n",
    "        GRADIENT_ACCUM = 8\n",
    "    else:\n",
    "        print(\"\\nüü° GPU STANDARD d√©tect√©: T4\")\n",
    "        BATCH_SIZE = 2\n",
    "        GRADIENT_ACCUM = 16\n",
    "    \n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Gradient accumulation: {GRADIENT_ACCUM}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA non disponible - Training sur CPU (tr√®s lent)\")\n",
    "    BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUM = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration - Authentification HF & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentification Hugging Face (pour push du mod√®le)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ‚ö†Ô∏è REMPLACER PAR VOTRE TOKEN HF (avec write access)\n",
    "HF_TOKEN = \"hf_...\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(\"‚úÖ Authentification HF r√©ussie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets depuis fichiers locaux\n",
    "# ‚ö†Ô∏è UPLOADER LES FICHIERS DANS COLAB :\n",
    "# - schemes_levelA_base.jsonl\n",
    "# - schemes_levelA_augmented.jsonl\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Charger les datasets\n",
    "dataset_base = load_dataset('json', data_files='schemes_levelA_base.jsonl', split='train')\n",
    "dataset_augmented = load_dataset('json', data_files='schemes_levelA_augmented.jsonl', split='train')\n",
    "\n",
    "# Combiner les datasets\n",
    "dataset_full = concatenate_datasets([dataset_base, dataset_augmented])\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√©: {len(dataset_full)} exemples\")\n",
    "print(f\"   - Base: {len(dataset_base)} exemples\")\n",
    "print(f\"   - Augment√©s: {len(dataset_augmented)} exemples\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nüìù Exemple de donn√©es:\")\n",
    "print(dataset_full[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/validation (95/5)\n",
    "dataset_split = dataset_full.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset_split['train']\n",
    "eval_dataset = dataset_split['test']\n",
    "\n",
    "print(f\"‚úÖ Split r√©alis√©:\")\n",
    "print(f\"   - Train: {len(train_dataset)} exemples\")\n",
    "print(f\"   - Validation: {len(eval_dataset)} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Mod√®le - Configuration QLoRA + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Configuration quantization 4-bit (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 (meilleure pr√©cision)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 optimal sur A100\n",
    "    bnb_4bit_use_double_quant=True,      # Double quantization (√©conomie VRAM)\n",
    ")\n",
    "\n",
    "print(\"üì• Chargement du mod√®le Mistral 7B...\")\n",
    "\n",
    "# Charger le mod√®le base en 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© en 4-bit\")\n",
    "\n",
    "# Pr√©parer le mod√®le pour training k-bit\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Mod√®le pr√©par√© pour k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration LoRA (rang 64 pour qualit√© maximale)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                              # Rang LoRA (64 = haute qualit√©)\n",
    "    lora_alpha=128,                    # Alpha = 2 * r (r√®gle empirique)\n",
    "    lora_dropout=0.05,                 # Dropout l√©ger\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[                   # Tous les modules attention + MLP\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher les param√®tres entra√Ænables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA appliqu√© (r={lora_config.r}, alpha={lora_config.lora_alpha})\")\n",
    "print(f\"   Param√®tres entra√Ænables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"   Param√®tres totaux: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training - Configuration & Lancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Configuration du training\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./mistral-7b-philosophes-lora\",\n",
    "    \n",
    "    # Batch size (ajust√© selon GPU)\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,              # Optimal pour LoRA\n",
    "    lr_scheduler_type=\"cosine\",      # Cosine avec warmup\n",
    "    warmup_ratio=0.03,               # 3% warmup\n",
    "    \n",
    "    # Epochs\n",
    "    num_train_epochs=3,              # 3 epochs pour √©viter overfitting\n",
    "    \n",
    "    # Optimisation\n",
    "    optim=\"paged_adamw_8bit\",        # AdamW 8-bit (√©conomie VRAM)\n",
    "    bf16=True,                        # bfloat16 sur A100\n",
    "    fp16=False,\n",
    "    \n",
    "    # Gradient clipping\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",                # Pas de W&B/Tensorboard\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration training cr√©√©e\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "print(f\"   Total steps: ~{len(train_dataset) * 3 // (BATCH_SIZE * GRADIENT_ACCUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de formatage des messages pour SFTTrainer\n",
    "def formatting_func(example):\n",
    "    \"\"\"Formate les messages au format ChatML pour Mistral\"\"\"\n",
    "    messages = example['messages']\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "# Cr√©er le trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,              # Longueur max des s√©quences\n",
    "    packing=False,                   # Pas de packing pour simplicit√©\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer cr√©√© et pr√™t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ LANCER LE TRAINING\n",
    "print(\"üöÄ D√©marrage du training...\\n\")\n",
    "\n",
    "# Afficher temps estim√©\n",
    "if \"A100\" in torch.cuda.get_device_name(0) and torch.cuda.get_device_properties(0).total_memory > 35e9:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 30-45 minutes (A100 40GB)\")\n",
    "elif \"V100\" in torch.cuda.get_device_name(0):\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 1h-1h30 (V100)\")\n",
    "else:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 2-3h (T4)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Export - Sauvegarde & Push vers HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le localement\n",
    "output_dir = \"./mistral-7b-philosophes-lora-final\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans: {output_dir}\")\n",
    "\n",
    "# Afficher la taille du LoRA\n",
    "import os\n",
    "lora_size = sum(os.path.getsize(os.path.join(output_dir, f)) for f in os.listdir(output_dir)) / 1024**2\n",
    "print(f\"   Taille LoRA: {lora_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push vers HF Hub (OPTIONNEL)\n",
    "# ‚ö†Ô∏è REMPLACER PAR VOTRE USERNAME HF\n",
    "HF_USERNAME = \"FJDaz\"\n",
    "REPO_NAME = f\"{HF_USERNAME}/mistral-7b-philosophes-lora\"\n",
    "\n",
    "print(f\"üì§ Push vers HF Hub: {REPO_NAME}...\")\n",
    "\n",
    "trainer.model.push_to_hub(\n",
    "    REPO_NAME,\n",
    "    use_auth_token=HF_TOKEN,\n",
    "    commit_message=\"Fine-tuned Mistral 7B on 1200 philosophy schemas (r=64)\"\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "    REPO_NAME,\n",
    "    use_auth_token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Mod√®le push√© sur: https://huggingface.co/{REPO_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test - Inf√©rence Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide du mod√®le fine-tun√©\n",
    "from transformers import pipeline\n",
    "\n",
    "# Charger le mod√®le fine-tun√©\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "# Test 1: Modus Ponens spinoziste\n",
    "test_prompt = \"\"\"Sch√®me : Modus Ponens\n",
    "Contexte : Si l'homme ignore les causes de ses passions, il est en servitude. Or l'√©l√®ve ignore les causes de ses passions.\n",
    "Applique le sch√®me :\"\"\"\n",
    "\n",
    "print(\"üìù Test 1: Modus Ponens spinoziste\")\n",
    "print(f\"Prompt: {test_prompt}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un tuteur philosophique ma√Ætrisant les sch√®mes logiques.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt}\n",
    "]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0]['generated_text'][-1]['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Identit√© spinoziste\n",
    "test_prompt_2 = \"\"\"Sch√®me : Identit√©\n",
    "Contexte : Dieu = Nature. Or la Nature est n√©cessaire.\n",
    "Applique le sch√®me :\"\"\"\n",
    "\n",
    "print(\"üìù Test 2: Identit√© spinoziste\")\n",
    "print(f\"Prompt: {test_prompt_2}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un tuteur philosophique ma√Ætrisant les sch√®mes logiques.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt_2}\n",
    "]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0]['generated_text'][-1]['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Tests termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Download - T√©l√©charger le LoRA Localement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipper le mod√®le pour t√©l√©chargement\n",
    "!zip -r mistral-7b-philosophes-lora-final.zip ./mistral-7b-philosophes-lora-final/\n",
    "\n",
    "print(\"‚úÖ Archive cr√©√©e: mistral-7b-philosophes-lora-final.zip\")\n",
    "print(\"üì• T√©l√©chargez le fichier depuis l'explorateur Colab (gauche)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checklist Finale\n",
    "\n",
    "- [ ] Training termin√© sans erreur\n",
    "- [ ] Eval loss < 0.6 (validation)\n",
    "- [ ] Tests d'inf√©rence corrects (application sch√®mes)\n",
    "- [ ] Mod√®le push√© sur HF Hub OU t√©l√©charg√© localement\n",
    "- [ ] Taille LoRA ~250-350 MB (r=64)\n",
    "\n",
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "1. **Benchmarks complets** : Tester sur 30 questions (10 par philosophe)\n",
    "2. **Test CPU** : Charger en 4-bit sur CPU, mesurer latence\n",
    "3. **D√©ploiement HF Space** : Cr√©er Space CPU gratuit avec ce LoRA\n",
    "4. **Comparaison** : vs Mistral 7B base (sans LoRA) + prompts syst√®me\n",
    "\n",
    "---\n",
    "\n",
    "**Cr√©√© le :** 20 novembre 2025  \n",
    "**Auteur :** Claude Code  \n",
    "**Mod√®le :** Mistral 7B Instruct v0.3 + LoRA (r=64, alpha=128)  \n",
    "**Dataset :** 1200 exemples sch√®mes philosophiques (Spinoza, Bergson, Kant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}