{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì Fine-Tuning Mistral 7B - Philosophes (LoRA)\n",
    "\n",
    "**Objectif :** Fine-tuner Mistral 7B sur 1200 exemples de sch√®mes logiques philosophiques\n",
    "\n",
    "**GPU Optimal :** A100 40GB (30-45 min) | V100 16GB (1h-1h30) | T4 15GB (2-3h)\n",
    "\n",
    "**Config :**\n",
    "- Mod√®le : `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "- M√©thode : QLoRA (4-bit) + LoRA (r=64, alpha=128)\n",
    "- Dataset : 1200 exemples (300 base + 900 augment√©s)\n",
    "- Epochs : 3\n",
    "- Batch size : 8 (A100) | 4 (V100/T4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup - Installation & V√©rifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Installation des d√©pendances optimis√©es (versions r√©centes)\n# Note: Versions sp√©cifiques pour √©viter conflits avec torchaudio/torchvision\nprint(\"üì¶ Installation des packages (peut prendre 2-3 minutes)...\\n\")\n\n!pip install -U \\\n    torch==2.8.0 torchvision==0.23.0 torchaudio==2.8.0 \\\n    transformers>=4.40.0 \\\n    peft>=0.10.0 \\\n    bitsandbytes>=0.43.0 \\\n    accelerate>=0.28.0 \\\n    trl>=0.8.0 \\\n    datasets>=2.18.0 \\\n    huggingface_hub>=0.22.0\n\nprint(\"\\n‚úÖ Installation termin√©e !\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rifier le GPU disponible\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"VRAM: {gpu_memory:.1f} GB\")\n",
    "    \n",
    "    # D√©terminer config optimale selon GPU\n",
    "    if \"A100\" in gpu_name and gpu_memory > 35:\n",
    "        print(\"\\nüöÄ GPU OPTIMAL d√©tect√©: A100 40GB\")\n",
    "        BATCH_SIZE = 8\n",
    "        GRADIENT_ACCUM = 4\n",
    "    elif \"V100\" in gpu_name or (\"A100\" in gpu_name and gpu_memory < 20):\n",
    "        print(\"\\n‚úÖ GPU EXCELLENT d√©tect√©: V100/A100-16GB\")\n",
    "        BATCH_SIZE = 4\n",
    "        GRADIENT_ACCUM = 8\n",
    "    else:\n",
    "        print(\"\\nüü° GPU STANDARD d√©tect√©: T4\")\n",
    "        BATCH_SIZE = 2\n",
    "        GRADIENT_ACCUM = 16\n",
    "    \n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"Gradient accumulation: {GRADIENT_ACCUM}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA non disponible - Training sur CPU (tr√®s lent)\")\n",
    "    BATCH_SIZE = 1\n",
    "    GRADIENT_ACCUM = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration - Authentification HF & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Authentification Hugging Face (pour push du mod√®le)\nfrom huggingface_hub import login\nfrom google.colab import userdata\n\n# Option 1: Utiliser les secrets Colab (RECOMMAND√â)\ntry:\n    HF_TOKEN = userdata.get('HF_TOKEN')\n    print(\"‚úÖ Token HF r√©cup√©r√© depuis Colab Secrets\")\nexcept:\n    # Option 2: Saisir manuellement si secret non d√©fini\n    print(\"‚ö†Ô∏è Token HF_TOKEN non trouv√© dans Colab Secrets\")\n    print(\"üìù Pour configurer le secret Colab:\")\n    print(\"   1. Cliquer sur l'ic√¥ne üîë (cl√©) dans la barre lat√©rale gauche\")\n    print(\"   2. Ajouter un nouveau secret: Nom = HF_TOKEN, Valeur = votre token HF\")\n    print(\"   3. Activer l'acc√®s pour ce notebook\")\n    print(\"   4. Red√©marrer ce notebook\\n\")\n    \n    # Fallback: saisir directement\n    from getpass import getpass\n    HF_TOKEN = getpass(\"Entrez votre token HF (obtenir sur https://huggingface.co/settings/tokens): \")\n\n# Login avec le token\nlogin(token=HF_TOKEN)\n\nprint(\"‚úÖ Authentification HF r√©ussie\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les datasets depuis fichiers locaux\n",
    "# ‚ö†Ô∏è UPLOADER LES FICHIERS DANS COLAB :\n",
    "# - schemes_levelA_base.jsonl\n",
    "# - schemes_levelA_augmented.jsonl\n",
    "\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Charger les datasets\n",
    "dataset_base = load_dataset('json', data_files='schemes_levelA_base.jsonl', split='train')\n",
    "dataset_augmented = load_dataset('json', data_files='schemes_levelA_augmented.jsonl', split='train')\n",
    "\n",
    "# Combiner les datasets\n",
    "dataset_full = concatenate_datasets([dataset_base, dataset_augmented])\n",
    "\n",
    "print(f\"‚úÖ Dataset charg√©: {len(dataset_full)} exemples\")\n",
    "print(f\"   - Base: {len(dataset_base)} exemples\")\n",
    "print(f\"   - Augment√©s: {len(dataset_augmented)} exemples\")\n",
    "\n",
    "# Afficher un exemple\n",
    "print(\"\\nüìù Exemple de donn√©es:\")\n",
    "print(dataset_full[0]['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split train/validation (95/5)\ndataset_split = dataset_full.train_test_split(test_size=0.05, seed=42)\ntrain_dataset = dataset_split['train']\neval_dataset = dataset_split['test']\n\nprint(f\"‚úÖ Split r√©alis√©:\")\nprint(f\"   - Train: {len(train_dataset)} exemples\")\nprint(f\"   - Validation: {len(eval_dataset)} exemples\")\n\n# Formater les datasets pour SFTTrainer (ajouter champ \"text\")\ndef format_chat_template(example):\n    \"\"\"Formate les messages au format ChatML et ajoute champ 'text'\"\"\"\n    messages = example['messages']\n    text = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    return {\"text\": text}\n\n# Appliquer le formatage\ntrain_dataset = train_dataset.map(format_chat_template, remove_columns=train_dataset.column_names)\neval_dataset = eval_dataset.map(format_chat_template, remove_columns=eval_dataset.column_names)\n\nprint(f\"\\n‚úÖ Datasets format√©s avec champ 'text'\")\nprint(f\"   Exemple de texte format√©:\")\nprint(train_dataset[0]['text'][:200] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Mod√®le - Configuration QLoRA + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Configuration quantization 4-bit (QLoRA)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",           # NormalFloat4 (meilleure pr√©cision)\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 optimal sur A100\n",
    "    bnb_4bit_use_double_quant=True,      # Double quantization (√©conomie VRAM)\n",
    ")\n",
    "\n",
    "print(\"üì• Chargement du mod√®le Mistral 7B...\")\n",
    "\n",
    "# Charger le mod√®le base en 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Charger le tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"‚úÖ Mod√®le charg√© en 4-bit\")\n",
    "\n",
    "# Pr√©parer le mod√®le pour training k-bit\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(\"‚úÖ Mod√®le pr√©par√© pour k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration LoRA (rang 64 pour qualit√© maximale)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,                              # Rang LoRA (64 = haute qualit√©)\n",
    "    lora_alpha=128,                    # Alpha = 2 * r (r√®gle empirique)\n",
    "    lora_dropout=0.05,                 # Dropout l√©ger\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[                   # Tous les modules attention + MLP\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Appliquer LoRA au mod√®le\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Afficher les param√®tres entra√Ænables\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"‚úÖ LoRA appliqu√© (r={lora_config.r}, alpha={lora_config.lora_alpha})\")\n",
    "print(f\"   Param√®tres entra√Ænables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"   Param√®tres totaux: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Training - Configuration & Lancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Configuration du training\n",
    "training_args = TrainingArguments(\n",
    "    # Output\n",
    "    output_dir=\"./mistral-7b-philosophes-lora\",\n",
    "    \n",
    "    # Batch size (ajust√© selon GPU)\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUM,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=2e-4,              # Optimal pour LoRA\n",
    "    lr_scheduler_type=\"cosine\",      # Cosine avec warmup\n",
    "    warmup_ratio=0.03,               # 3% warmup\n",
    "    \n",
    "    # Epochs\n",
    "    num_train_epochs=3,              # 3 epochs pour √©viter overfitting\n",
    "    \n",
    "    # Optimisation\n",
    "    optim=\"paged_adamw_8bit\",        # AdamW 8-bit (√©conomie VRAM)\n",
    "    bf16=True,                        # bfloat16 sur A100\n",
    "    fp16=False,\n",
    "    \n",
    "    # Gradient clipping\n",
    "    max_grad_norm=0.3,\n",
    "    \n",
    "    # Logging & Evaluation\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Misc\n",
    "    report_to=\"none\",                # Pas de W&B/Tensorboard\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Configuration training cr√©√©e\")\n",
    "print(f\"   Effective batch size: {BATCH_SIZE * GRADIENT_ACCUM}\")\n",
    "print(f\"   Total steps: ~{len(train_dataset) * 3 // (BATCH_SIZE * GRADIENT_ACCUM)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cr√©er le trainer (API simplifi√©e trl>=0.8.0)\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    processing_class=tokenizer,\n)\n\nprint(\"‚úÖ Trainer cr√©√© et pr√™t\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ LANCER LE TRAINING\n",
    "print(\"üöÄ D√©marrage du training...\\n\")\n",
    "\n",
    "# Afficher temps estim√©\n",
    "if \"A100\" in torch.cuda.get_device_name(0) and torch.cuda.get_device_properties(0).total_memory > 35e9:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 30-45 minutes (A100 40GB)\")\n",
    "elif \"V100\" in torch.cuda.get_device_name(0):\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 1h-1h30 (V100)\")\n",
    "else:\n",
    "    print(\"‚è±Ô∏è Temps estim√©: 2-3h (T4)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Training\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Export - Sauvegarde & Push vers HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le mod√®le localement\n",
    "output_dir = \"./mistral-7b-philosophes-lora-final\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Mod√®le sauvegard√© dans: {output_dir}\")\n",
    "\n",
    "# Afficher la taille du LoRA\n",
    "import os\n",
    "lora_size = sum(os.path.getsize(os.path.join(output_dir, f)) for f in os.listdir(output_dir)) / 1024**2\n",
    "print(f\"   Taille LoRA: {lora_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Push vers HF Space 3_PHI (RECOMMAND√â - mise √† jour directe du Space)\nSPACE_REPO = \"FJDaz/3_PHI\"\n\nprint(f\"üì§ Push du LoRA vers Space HF: {SPACE_REPO}...\")\nprint(\"‚ö†Ô∏è Le LoRA sera ajout√© au Space existant (ne remplace pas le mod√®le base)\")\n\n# Cr√©er un sous-dossier \"lora\" dans le Space pour le LoRA\ntrainer.model.push_to_hub(\n    SPACE_REPO,\n    use_auth_token=HF_TOKEN,\n    commit_message=\"Add Mistral 7B LoRA fine-tuned on 1200 philosophy schemas (r=64)\",\n    subfolder=\"lora\",  # Push dans un sous-dossier pour ne pas √©craser le Space\n)\n\ntokenizer.push_to_hub(\n    SPACE_REPO,\n    use_auth_token=HF_TOKEN,\n    subfolder=\"lora\",\n)\n\nprint(f\"‚úÖ LoRA push√© sur Space: https://huggingface.co/spaces/{SPACE_REPO}\")\nprint(f\"   Emplacement: https://huggingface.co/spaces/{SPACE_REPO}/tree/main/lora\")\nprint(\"\\nüí° Pour utiliser le LoRA dans le Space:\")\nprint(\"   1. Modifier app.py pour charger depuis './lora/'\")\nprint(\"   2. PeftModel.from_pretrained(model, './lora/')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Test - Inf√©rence Rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test rapide du mod√®le fine-tun√©\nimport torch\n\nprint(\"üìù Test 1: Modus Ponens spinoziste\")\n\ntest_prompt = \"\"\"Sch√®me : Modus Ponens\nContexte : Si l'homme ignore les causes de ses passions, il est en servitude. Or l'√©l√®ve ignore les causes de ses passions.\nApplique le sch√®me :\"\"\"\n\nprint(f\"Prompt: {test_prompt}\")\nprint(\"\\n\" + \"=\"*60)\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"Tu es un tuteur philosophique ma√Ætrisant les sch√®mes logiques.\"},\n    {\"role\": \"user\", \"content\": test_prompt}\n]\n\n# Formater avec le tokenizer\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize=True,\n    add_generation_prompt=True,\n    return_tensors=\"pt\"\n).to(model.device)\n\n# Utiliser autocast pour g√©rer automatiquement les types mixtes (BFloat16/Float32)\nprint(\"üîß G√©n√©ration avec torch.autocast(bfloat16)...\")\nwith torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=128,\n            do_sample=True,\n            temperature=0.7,\n            pad_token_id=tokenizer.eos_token_id,\n        )\n\n# D√©coder la r√©ponse\nresponse = tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)\nprint(response)\n\nprint(\"\\n\" + \"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Identit√© spinoziste\n",
    "test_prompt_2 = \"\"\"Sch√®me : Identit√©\n",
    "Contexte : Dieu = Nature. Or la Nature est n√©cessaire.\n",
    "Applique le sch√®me :\"\"\"\n",
    "\n",
    "print(\"üìù Test 2: Identit√© spinoziste\")\n",
    "print(f\"Prompt: {test_prompt_2}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Tu es un tuteur philosophique ma√Ætrisant les sch√®mes logiques.\"},\n",
    "    {\"role\": \"user\", \"content\": test_prompt_2}\n",
    "]\n",
    "\n",
    "output = pipe(messages)\n",
    "print(output[0]['generated_text'][-1]['content'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Tests termin√©s !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Download - T√©l√©charger le LoRA Localement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zipper le mod√®le pour t√©l√©chargement\n",
    "!zip -r mistral-7b-philosophes-lora-final.zip ./mistral-7b-philosophes-lora-final/\n",
    "\n",
    "print(\"‚úÖ Archive cr√©√©e: mistral-7b-philosophes-lora-final.zip\")\n",
    "print(\"üì• T√©l√©chargez le fichier depuis l'explorateur Colab (gauche)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Checklist Finale\n",
    "\n",
    "- [ ] Training termin√© sans erreur\n",
    "- [ ] Eval loss < 0.6 (validation)\n",
    "- [ ] Tests d'inf√©rence corrects (application sch√®mes)\n",
    "- [ ] Mod√®le push√© sur HF Hub OU t√©l√©charg√© localement\n",
    "- [ ] Taille LoRA ~250-350 MB (r=64)\n",
    "\n",
    "## üéØ Prochaines √âtapes\n",
    "\n",
    "1. **Benchmarks complets** : Tester sur 30 questions (10 par philosophe)\n",
    "2. **Test CPU** : Charger en 4-bit sur CPU, mesurer latence\n",
    "3. **D√©ploiement HF Space** : Cr√©er Space CPU gratuit avec ce LoRA\n",
    "4. **Comparaison** : vs Mistral 7B base (sans LoRA) + prompts syst√®me\n",
    "\n",
    "---\n",
    "\n",
    "**Cr√©√© le :** 20 novembre 2025  \n",
    "**Auteur :** Claude Code  \n",
    "**Mod√®le :** Mistral 7B Instruct v0.3 + LoRA (r=64, alpha=128)  \n",
    "**Dataset :** 1200 exemples sch√®mes philosophiques (Spinoza, Bergson, Kant)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}