# üìã Synth√®se Projet - Fine-Tuning Mistral 7B Philosophes

**Date :** 20 novembre 2025
**Objectif :** Cr√©er une version CPU-compatible de Bergson and Friends pour strat√©gie freemium

---

## üéØ Contexte

### Probl√®me Initial
- **HF Spaces GPU coup√©s** ‚Üí SNB 14B (Qwen + LoRA Spinoza) impossible sur CPU gratuit
- **Latence CPU prohibitive** ‚Üí Qwen 14B n√©cessite GPU (~$18-60/mois ou pay-per-use Modal)

### Solution Propos√©e
**Fine-tuner Mistral 7B sur sch√®mes logiques** pour cr√©er un tier gratuit viable :
- Mistral 7B plus efficace sur CPU que Qwen (optimisations internes)
- LoRA sp√©cialis√© sch√®mes logiques ‚Üí compense taille mod√®le r√©duite
- 1200 exemples disponibles (sch√®mes Niveau A)

---

## üìä Architecture Freemium

| Crit√®re | Free Tier | Premium Tier |
|---------|-----------|--------------|
| **Mod√®le** | Mistral 7B + LoRA sch√®mes | Qwen 14B + LoRA SNB |
| **Infrastructure** | HF Space CPU (gratuit) | Modal GPU A10G (pay-per-use) |
| **Qualit√©** | üü° Bonne (80% accuracy) | ‚úÖ Excellente (95%+ accuracy) |
| **Latence** | ‚ö†Ô∏è 5-15s par r√©ponse | ‚úÖ 1-3s par r√©ponse |
| **Prompts** | Syst√®me seul (sch√®mes dans LoRA) | Syst√®me + RAG enrichi |
| **Co√ªt** | ‚úÖ Gratuit | üí∞ ~$0.50-1/h utilisation |

**Strat√©gie :**
- Free tier = **teaser** fonctionnel (attire utilisateurs)
- Premium tier = **qualit√© + rapidit√©** (justifie paiement)

---

## üß¨ Dataset Fine-Tuning

### Source
- **Localisation :** `/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/data/FT/processed/`
- **Fichiers :**
  - `schemes_levelA_base.jsonl` (300 exemples)
  - `schemes_levelA_augmented.jsonl` (900 exemples)
- **Total :** 1200 exemples

### Contenu
**Format :** ChatML (system/user/assistant)

**Sch√®mes logiques couverts :**
- **Modus Ponens** (Si P alors Q, or P, donc Q)
- **Identit√©** (Dieu = Nature, Libert√© = Connaissance n√©cessit√©)
- **Causalit√©** (Tout a une cause, joie ‚Üí puissance)
- **Opposition** (Dur√©e ‚â† temps spatial)
- **Analogie** (Conscience = m√©lodie)
- **Distinction** (Ph√©nom√®ne ‚â† noum√®ne)

**Philosophes :**
- Spinoza : 600 exemples
- Bergson : 300 exemples
- Kant : 300 exemples

**Registre :** Lyc√©en (Terminale) - vocabulaire accessible, phrases courtes

---

## üîß Configuration Training Optimale

### Mod√®le Base
- **Nom :** `mistralai/Mistral-7B-Instruct-v0.3`
- **Raison :** Optimis√© CPU, excellente qualit√© 7B, support long context

### M√©thode Fine-Tuning
- **QLoRA** (4-bit quantization + LoRA)
- **Rang LoRA :** r=64 (haute qualit√© vs r=8/16 standard)
- **Alpha LoRA :** 128 (= 2 * r)
- **Modules cibl√©s :** Attention (q/k/v/o) + MLP (gate/up/down)

### Hyperparam√®tres
- **Batch size effectif :** 32 (8 x 4 grad accum sur A100)
- **Learning rate :** 2e-4 (optimal LoRA)
- **Scheduler :** Cosine avec 3% warmup
- **Epochs :** 3 (√©vite overfitting sur 1200 exemples)
- **Pr√©cision :** bfloat16 (optimal A100)

### Infrastructure
- **GPU optimal :** A100 40GB (Colab Pro)
- **Temps training :** 30-45 minutes
- **Fallback :** V100 16GB (1h-1h30) | T4 15GB (2-3h)

---

## üì¶ Livrables Cr√©√©s

### Documentation
1. **README.md** - Vue d'ensemble projet
2. **QUICKSTART.md** - Guide rapide 3 √©tapes (5 min setup)
3. **USAGE.md** - Documentation compl√®te (troubleshooting, benchmarks)
4. **PROJECT_SUMMARY.md** - Ce document (synth√®se projet)

### Code
1. **notebooks/train_mistral_7b_lora.ipynb** - Notebook Colab cl√© en main
2. **scripts/test_model.py** - Script benchmarks local (10 questions test)
3. **configs/mistral_7b_lora.yaml** - Configuration hyperparam√®tres

### Infrastructure
1. **requirements.txt** - D√©pendances Python
2. **.gitignore** - Exclusions Git (mod√®les, cache)
3. **.gitattributes** - Git LFS (si commit checkpoints)

---

## üéØ Prochaines √âtapes

### Imm√©diat (Vous)
1. ‚úÖ **Upload notebook sur Colab Pro**
2. ‚úÖ **Lancer training** (30-45 min sur A100)
3. ‚úÖ **Download LoRA** (250-350 MB)

### Court Terme (Apr√®s Training)
4. **Benchmarks complets** (scripts/test_model.py)
   - 10 questions par philosophe
   - Mesurer accuracy + latence CPU
5. **Comparaison baseline** (Mistral 7B sans LoRA vs avec LoRA)
6. **D√©ploiement HF Space CPU** (test freemium tier)

### Moyen Terme (Validation Strat√©gie)
7. **Tests utilisateurs** (√©chantillon lyc√©ens sur free tier)
8. **Comparaison premium** (Mistral 7B CPU vs Qwen 14B GPU)
9. **D√©cision finale** freemium strategy

---

## üìä R√©sultats Attendus

### Training
- **Training loss :** < 0.5
- **Eval loss :** < 0.6
- **Taille LoRA :** ~250-350 MB
- **Param√®tres entra√Ænables :** ~1-2% du total

### Inf√©rence (CPU 4-bit)
- **Accuracy sch√®mes :** > 80%
- **Latence :** 5-15s par r√©ponse
- **RAM requise :** ~4 GB

### Validation Freemium
- **Free tier viable ?** Oui si latence <15s + accuracy >75%
- **Premium justifi√© ?** Oui si √©cart qualit√©/latence significatif vs free

---

## üîç Questions Ouvertes

1. **Mistral 7B LoRA peut-il rivaliser avec prompts syst√®me seuls ?**
   ‚Üí Test ablation : LoRA vs prompts vs LoRA+prompts

2. **Latence CPU acceptable pour lyc√©ens ?**
   ‚Üí Seuil psychologique : 10-15s (dialogue p√©dagogique)

3. **Quelle config freemium finale ?**
   - Option A : Free CPU + Premium GPU Modal
   - Option B : Free prompts seuls + Premium LoRA GPU
   - Option C : Pas de free tier, seulement premium abordable

---

## üí° Insights Strat√©giques

### Pourquoi LoRA plut√¥t que Prompts Seuls ?

**Avantages LoRA :**
- ‚úÖ Application **native** des sch√®mes (appris dans les poids)
- ‚úÖ Moins de tokens prompt ‚Üí **latence r√©duite**
- ‚úÖ Vocabulaire lyc√©en **internalis√©**

**Inconv√©nient LoRA :**
- ‚ö†Ô∏è N√©cessite training (~1h)
- ‚ö†Ô∏è Moins flexible que prompts (modification = re-training)

**Compromis :** LoRA (sch√®mes) + Prompts syst√®me (personnalit√© philosophe)

### Pourquoi Mistral 7B plut√¥t que Phi-3 / Qwen 7B ?

| Mod√®le | Avantages | Inconv√©nients |
|--------|-----------|---------------|
| **Mistral 7B** | Optimisations CPU, qualit√© excellente, long context | Pas sp√©cialis√© philo |
| Qwen 7B | Bon sur philo, multilingue | Plus lent CPU que Mistral |
| Phi-3.5-mini (3.8B) | Ultra rapide CPU | Qualit√© moindre (3.8B) |

**Verdict :** Mistral 7B = **meilleur compromis** qualit√©/performance CPU

---

## üìû Contact & Support

**Projet :** Bergson and Friends - Freemium Strategy
**Repo :** `/Users/francois-jeandazin/NUX_FT/bergsonAndFriends`
**Auteur :** Fran√ßois-Jean d'Azin
**Assistant :** Claude Code (Anthropic)

**Ressources :**
- Notebook Colab : `notebooks/train_mistral_7b_lora.ipynb`
- Guide rapide : `QUICKSTART.md`
- Documentation : `USAGE.md`
- Config : `configs/mistral_7b_lora.yaml`

---

**Derni√®re mise √† jour :** 20 novembre 2025 - 11:25
**Status :** Setup complet - Pr√™t pour training Colab Pro
