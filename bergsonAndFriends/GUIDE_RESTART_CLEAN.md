# üöÄ Guide Restart Propre - Mistral 7B Fine-Tuning

**Date :** 20 novembre 2025
**Objectif :** Relancer le fine-tuning proprement avec dataset combin√© (80% sch√®mes + 20% incarnation)

---

## üì¶ Fichiers √† Pr√©parer

### 1. Notebook propre
‚úÖ **Cr√©√©** : `/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/notebooks/train_mistral_7b_lora_CLEAN.ipynb`

### 2. Datasets √† uploader
Localisation : `/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/data/FT/processed/`

```bash
# V√©rifier que les fichiers existent
ls -lh /Users/francois-jeandazin/NUX_FT/bergsonAndFriends/data/FT/processed/*.jsonl
```

**Fichiers requis :**
- ‚úÖ `schemes_levelA_base.jsonl` (~545 KB, 300 exemples)
- ‚úÖ `schemes_levelA_augmented.jsonl` (~1.6 MB, 900 exemples)
- ‚úÖ `enriched_correction_dataset.jsonl` (~100 KB, 213 exemples)

---

## üéØ Plan d'Action (5 √©tapes)

### √âTAPE 1 : Nettoyer Colab (2 min)

1. Si Colab est ouvert : **Runtime > Disconnect and delete runtime**
2. Fermer l'onglet Colab
3. Attendre 30 secondes

### √âTAPE 2 : Upload notebook (1 min)

1. Aller sur [Google Colab](https://colab.research.google.com/)
2. **File > Upload notebook**
3. S√©lectionner : `/Users/francois-jeandazin/NUX_FT/bergsonAndFriends/notebooks/train_mistral_7b_lora_CLEAN.ipynb`
4. **Runtime > Change runtime type > GPU = L4** (ou A100 si dispo)

### √âTAPE 3 : Upload datasets (2 min)

Dans Colab, panneau **Files** (ic√¥ne üìÅ √† gauche) :

1. Cliquer sur **Upload to session storage** (ic√¥ne upload)
2. S√©lectionner les 3 fichiers JSONL :
   - `schemes_levelA_base.jsonl`
   - `schemes_levelA_augmented.jsonl`
   - `enriched_correction_dataset.jsonl`
3. Attendre que les 3 fichiers apparaissent dans `/content/`

**V√©rification :**
```python
!ls -lh /content/*.jsonl
```

Doit afficher :
```
-rw-r--r-- 1 root root 545K enriched_correction_dataset.jsonl
-rw-r--r-- 1 root root 100K schemes_levelA_augmented.jsonl
-rw-r--r-- 1 root root 1.6M schemes_levelA_base.jsonl
```

### √âTAPE 4 : Configurer token HF (1 min)

**Option A : Colab Secrets (RECOMMAND√â)**

1. Cliquer sur l'ic√¥ne **üîë** (Secrets) dans la barre gauche
2. Cliquer **+ Add new secret**
3. Name: `HF_TOKEN`
4. Value: `hf_...` (votre token HF avec write access)
5. Toggle : **Activer l'acc√®s** pour ce notebook

**Option B : Saisie manuelle**

Le notebook vous demandera le token si Secrets non configur√©.

### √âTAPE 5 : Lancer le training (3-4h total)

**Ex√©cution s√©quentielle :**

```
Section 1  : Installation packages        [3 min]
Section 2  : Authentification HF          [10 sec]
Section 3  : Chargement datasets          [30 sec]
Section 4  : Chargement mod√®le + LoRA     [10 min]
Section 5  : Training INITIAL             [2-3h sur L4] ‚ö†Ô∏è LONG
Section 6  : Sauvegarde checkpoint        [1 min]
Section 7  : Test sch√®mes                 [30 sec]
Section 8  : Re-fine-tuning COMBIN√â       [1-1.5h sur L4]
Section 9  : Test dialogue interactif     [illimit√©]
```

**Comment lancer :**

- **Option rapide** : `Runtime > Run all` (lance tout d'un coup)
- **Option prudente** : Ex√©cuter cellule par cellule (Shift+Enter)

---

## ‚è±Ô∏è Temps Estim√©s par GPU

| GPU | Section 5 (initial) | Section 8 (combin√©) | **TOTAL** |
|-----|---------------------|---------------------|-----------|
| **A100 40GB** | 30-45 min | 30 min | **~1h** |
| **V100 16GB** | 1h-1h30 | 45 min | **~2h15** |
| **L4 15GB** | 2-3h | 1-1.5h | **~3-4h** |
| **T4 15GB** | 2-3h | 1-1.5h | **~3-4h** |

---

## üìä Monitoring du Training

### M√©triques √† surveiller

**Section 5 (initial) :**
```
Step 50  : loss=1.234, eval_loss=1.567
Step 100 : loss=0.890, eval_loss=1.123
...
Step 300 : loss=0.456, eval_loss=0.678  ‚Üê Objectif final
```

**Objectifs :**
- ‚úÖ `eval_loss` < 0.6 (validation OK)
- ‚úÖ `eval_loss` diminue progressivement
- ‚ö†Ô∏è Si `eval_loss` remonte ‚Üí overfitting (mais load_best_model_at_end g√®re)

**Section 8 (combin√©) :**
- M√™mes objectifs
- Surveillance toutes les 20 steps (plus fr√©quent)

---

## ‚úÖ V√©rifications Post-Training

### Apr√®s Section 7 (test sch√®mes)

**Attendu :**
```
R√©ponse: Donc l'√©l√®ve est en servitude.
```

**Probl√®me si :**
- R√©pond hors sujet
- N'applique pas le sch√®me
- G√©n√®re du charabia

### Apr√®s Section 9 (dialogue interactif)

**Tests √† faire :**

1. **Test 1√®re personne :**
   ```
   VOUS : Ben, c'est pas toi Spinoza ?
   SPINOZA : Oui, je suis Spinoza. Je te parle en premi√®re personne. [OK]
   SPINOZA : Oui, c'est Spinoza. Pour Spinoza, ... [‚ùå 3√®me personne]
   ```

2. **Test sch√®mes :**
   ```
   VOUS : La libert√©, c'est faire ce qu'on veut ?
   SPINOZA : Pas vraiment. Mais alors, si tu fais ce que tu veux sans comprendre pourquoi... [OK]
   ```

3. **Test r√©p√©tition :**
   - Poser 3 questions diff√©rentes
   - V√©rifier que les r√©ponses varient

---

## üéØ R√©sultats Attendus

### Checkpoint final

**Localisation Colab :**
- `/content/mistral-combined-final/` (250-350 MB)

**Fichiers :**
```
mistral-combined-final/
‚îú‚îÄ‚îÄ adapter_config.json
‚îú‚îÄ‚îÄ adapter_model.safetensors  (~300 MB)
‚îî‚îÄ‚îÄ tokenizer files
```

### Push Hugging Face

**URL :** `https://huggingface.co/spaces/FJDaz/3_PHI/tree/main/Spinoza_Secours`

**Structure finale HF Space :**
```
FJDaz/3_PHI/
‚îú‚îÄ‚îÄ qwen-spinoza-niveau-b/   ‚Üê LoRA SNB original (INCHANG√â)
‚îî‚îÄ‚îÄ Spinoza_Secours/         ‚Üê LoRA Mistral 7B nouveau
    ‚îú‚îÄ‚îÄ adapter_config.json
    ‚îú‚îÄ‚îÄ adapter_model.safetensors
    ‚îî‚îÄ‚îÄ tokenizer files
```

---

## ‚ö†Ô∏è Troubleshooting

### Erreur : "FileNotFoundError: schemes_levelA_base.jsonl"

**Cause :** Fichiers JSONL non upload√©s dans Colab

**Solution :**
1. V√©rifier panneau Files (gauche)
2. Re-uploader les 3 fichiers JSONL
3. Relancer la cellule

### Erreur : "RuntimeError: CUDA out of memory"

**Cause :** GPU trop petit ou batch size trop grand

**Solution :**
```python
# Dans une nouvelle cellule, avant Section 5
BATCH_SIZE = 1
GRADIENT_ACCUM = 32
```

Puis relancer les sections 5 et 8.

### Training bloqu√© (pas de progression)

**Sympt√¥mes :**
- Cellule tourne depuis >20 min
- Pas de barres de progression
- Pas de logs

**Solution :**
1. **Stop** (carr√© √† gauche de la cellule)
2. Si bloqu√© : `Runtime > Interrupt execution`
3. Si encore bloqu√© : `Runtime > Disconnect and delete runtime`
4. Recommencer depuis √âTAPE 1

### eval_loss > 1.0 (ne converge pas)

**Cause :** Learning rate trop √©lev√© ou dataset corrompu

**Solution :**
```python
# Modifier dans Section 5 et 8
learning_rate=1e-4,  # Au lieu de 2e-4
```

---

## üìû Support

**Documentation compl√®te :**
- [README.md](README.md) - Vue d'ensemble
- [QUICKSTART.md](QUICKSTART.md) - Guide rapide
- [USAGE.md](USAGE.md) - Guide d√©taill√©
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Probl√®mes courants
- [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) - Synth√®se projet

**En cas de blocage :**
1. V√©rifier [TROUBLESHOOTING.md](TROUBLESHOOTING.md)
2. Copier l'erreur compl√®te
3. Demander √† Claude Code

---

## üéì Apr√®s le Training

### T√©l√©charger le mod√®le localement

```python
# Derni√®re cellule du notebook
!zip -r mistral-combined-final.zip /content/mistral-combined-final/
```

Puis : **Files > Download** `mistral-combined-final.zip`

### Tester localement

```bash
cd /Users/francois-jeandazin/NUX_FT/bergsonAndFriends
unzip mistral-combined-final.zip

python scripts/test_model.py --lora ./mistral-combined-final
```

---

**Derni√®re mise √† jour :** 20 novembre 2025 - 14:30
**Version notebook :** CLEAN v2
**Status :** ‚úÖ Pr√™t pour training
